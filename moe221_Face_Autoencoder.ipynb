{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "moe221_Face_Autoencoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moe221/diversity_in_cinema/blob/moe221_face_autoencoder/moe221_Face_Autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc": true,
        "id": "pFTH7ytz09jA"
      },
      "source": [
        "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
        "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Prototyping-VGG16\" data-toc-modified-id=\"Prototyping-VGG16-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Prototyping VGG16</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.-Mount-Google-Storage-Bucket\" data-toc-modified-id=\"1.-Mount-Google-Storage-Bucket-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>1. Mount Google Storage Bucket</a></span></li><li><span><a href=\"#2.-Transfer-Learning\" data-toc-modified-id=\"2.-Transfer-Learning-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>2. Transfer Learning</a></span></li></ul></li></ul></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QCKrCbbYWSy"
      },
      "source": [
        "# Face Autoencoder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgOvtDCsYWlQ"
      },
      "source": [
        "## 1. Mount Google Storage Bucket "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBoSucAljj42"
      },
      "source": [
        "# project id - GCP project id\n",
        "PROJECT_ID=\"le-wagon-bootcamp-328018\"\n",
        "\n",
        "# bucket name - GCP bucket name\n",
        "BUCKET_NAME=\"diversity-in-cinema-735\"\n",
        "\n",
        "# train data file location\n",
        "BUCKET_TRAIN_DATA_PATH = \"data/training_data\"\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PII43_d6YW5J"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yz5FPNOjVn3g",
        "outputId": "f26e77d7-4f40-41d0-8432-ac0e642899a3"
      },
      "source": [
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "!apt -qq update\n",
        "!apt -qq install gcsfuse"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  2537  100  2537    0     0  65051      0 --:--:-- --:--:-- --:--:-- 65051\n",
            "OK\n",
            "42 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "The following NEW packages will be installed:\n",
            "  gcsfuse\n",
            "0 upgraded, 1 newly installed, 0 to remove and 42 not upgraded.\n",
            "Need to get 11.3 MB of archives.\n",
            "After this operation, 24.0 MB of additional disk space will be used.\n",
            "Selecting previously unselected package gcsfuse.\n",
            "(Reading database ... 155222 files and directories currently installed.)\n",
            "Preparing to unpack .../gcsfuse_0.37.0_amd64.deb ...\n",
            "Unpacking gcsfuse (0.37.0) ...\n",
            "Setting up gcsfuse (0.37.0) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URkhtScZV1EF",
        "outputId": "150ac1fd-c574-4d89-a188-6c62188dcad7"
      },
      "source": [
        "!mkdir gcp\n",
        "!gcsfuse --implicit-dirs diversity-in-cinema-735 gcp"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021/11/30 21:21:44.131542 Start gcsfuse/0.37.0 (Go version go1.17.2) for app \"\" using mount point: /content/gcp\n",
            "2021/11/30 21:21:44.141345 Opening GCS connection...\n",
            "2021/11/30 21:21:44.566764 Mounting file system \"diversity-in-cinema-735\"...\n",
            "2021/11/30 21:21:44.605440 File system has been successfully mounted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdqO-N_4XFhz"
      },
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, BatchNormalization, LeakyReLU, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint \n",
        "from tensorflow.keras.utils import plot_model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikue2NiG2q-V"
      },
      "source": [
        "\n",
        "!pip install -U -q kaggle\n",
        "!mkdir -p ~/.kaggle"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "IHlahRVc2nl_",
        "outputId": "83c0221b-9bc0-42b0-df47-a1fdf1aa513b"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b758d87c-733e-4bdf-bcda-5aa216687c72\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b758d87c-733e-4bdf-bcda-5aa216687c72\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDTvrplx2iZg"
      },
      "source": [
        "!cp kaggle.json ~/.kaggle/\n",
        "!kaggle datasets download -d jessicali9530/celeba-dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70LtJnQgEM6c"
      },
      "source": [
        "import os\n",
        "from glob import glob\n",
        "\n",
        "WEIGHTS_FOLDER = './weights/'\n",
        "DATA_FOLDER = './data/img_align_celeba/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12QkJczL4QcX"
      },
      "source": [
        "if not os.path.exists(WEIGHTS_FOLDER):\n",
        "  os.makedirs(os.path.join(WEIGHTS_FOLDER,\"AE\"))\n",
        "  os.makedirs(os.path.join(WEIGHTS_FOLDER,\"VAE\"))\n",
        "\n",
        "#Unzip the dataset downloaded from kaggle\n",
        "from zipfile import ZipFile\n",
        "with ZipFile('celeba-dataset.zip', 'r') as zipObj:\n",
        "   # Extract all the contents of zip file in the data directory\n",
        "   zipObj.extractall('./data/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlAcEt0X2XQj",
        "outputId": "ec8f426a-8a64-4555-bfd3-0235607a7c57"
      },
      "source": [
        "filenames = np.array(glob(os.path.join(DATA_FOLDER, '*/*.jpg')))\n",
        "NUM_IMAGES = len(filenames)\n",
        "print(\"Total number of images : \" + str(NUM_IMAGES))\n",
        "# prints : Total number of images : 202599\n",
        "\n",
        "\n",
        "INPUT_DIM = (128,128,3) # Image dimension\n",
        "BATCH_SIZE = 512\n",
        "Z_DIM = 200 # Dimension of the latent vector (z)\n",
        "\n",
        "data_flow = ImageDataGenerator(rescale=1./255).flow_from_directory(DATA_FOLDER, \n",
        "                                                                   target_size = INPUT_DIM[:2],\n",
        "                                                                   batch_size = BATCH_SIZE,\n",
        "                                                                   shuffle = True,\n",
        "                                                                   class_mode = 'input',\n",
        "                                                                   subset = 'training')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of images : 202599\n",
            "Found 202599 images belonging to 1 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "id": "vfUngmfA9F7p",
        "outputId": "4cf91d8e-1f51-4da5-8d86-26bfbfae27ca"
      },
      "source": [
        "# load data using generator\n",
        "data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "print(\"loading training set\")\n",
        "\n",
        "train_generator = data_generator.flow_from_dataframe(\n",
        "    train_df.iloc[:2000, :], \n",
        "    \"gcp/data/training_data/\", \n",
        "    x_col='file',\n",
        "    y_col='gender',\n",
        "    target_size=(224, 224),\n",
        "    class_mode='categorical',\n",
        "    batch_size=15)\n",
        "\n",
        "print(\"loading validation set\")\n",
        "\n",
        "validation_generator = data_generator.flow_from_dataframe(\n",
        "    test_df.iloc[:1000, :], \n",
        "    \"gcp/data/training_data/\", \n",
        "    x_col='file',\n",
        "    y_col='gender',\n",
        "    target_size=(224, 224),\n",
        "    class_mode='categorical',\n",
        "    batch_size=15)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-8af2a70c9bf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load data using generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessing_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocess_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading training set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'preprocess_input' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Da9_kU0f5XJt"
      },
      "source": [
        "\n",
        "# ENCODER\n",
        "def build_vae_encoder(input_dim, output_dim, conv_filters, conv_kernel_size, \n",
        "                  conv_strides, use_batch_norm = False, use_dropout = False):\n",
        "  \n",
        "  # Clear tensorflow session to reset layer index numbers to 0 for LeakyRelu, \n",
        "  # BatchNormalization and Dropout.\n",
        "  # Otherwise, the names of above mentioned layers in the model \n",
        "  # would be inconsistent\n",
        "  global K\n",
        "  K.clear_session()\n",
        "  \n",
        "  # Number of Conv layers\n",
        "  n_layers = len(conv_filters)\n",
        "\n",
        "  # Define model input\n",
        "  encoder_input = Input(shape = input_dim, name = 'encoder_input')\n",
        "  x = encoder_input\n",
        "\n",
        "  # Add convolutional layers\n",
        "  for i in range(n_layers):\n",
        "      x = Conv2D(filters = conv_filters[i], \n",
        "                  kernel_size = conv_kernel_size[i],\n",
        "                  strides = conv_strides[i], \n",
        "                  padding = 'same',\n",
        "                  name = 'encoder_conv_' + str(i)\n",
        "                  )(x)\n",
        "      if use_batch_norm:\n",
        "        x = BathcNormalization()(x)\n",
        "  \n",
        "      x = LeakyReLU()(x)\n",
        "\n",
        "      if use_dropout:\n",
        "        x = Dropout(rate=0.25)(x)\n",
        "\n",
        "  # Required for reshaping latent vector while building Decoder\n",
        "  shape_before_flattening = K.int_shape(x)[1:] \n",
        "  \n",
        "  x = Flatten()(x)\n",
        "  \n",
        "  mean_mu = Dense(output_dim, name = 'mu')(x)\n",
        "  log_var = Dense(output_dim, name = 'log_var')(x)\n",
        "\n",
        "  # Defining a function for sampling\n",
        "  def sampling(args):\n",
        "    mean_mu, log_var = args\n",
        "    epsilon = K.random_normal(shape=K.shape(mean_mu), mean=0., stddev=1.) \n",
        "    return mean_mu + K.exp(log_var/2)*epsilon   \n",
        "  \n",
        "  # Using a Keras Lambda Layer to include the sampling function as a layer \n",
        "  # in the model\n",
        "  encoder_output = Lambda(sampling, name='encoder_output')([mean_mu, log_var])\n",
        "\n",
        "  return encoder_input, encoder_output, mean_mu, log_var, shape_before_flattening, Model(encoder_input, encoder_output)\n",
        "\n",
        "vae_encoder_input, vae_encoder_output,  mean_mu, log_var, vae_shape_before_flattening, vae_encoder  = build_vae_encoder(input_dim = INPUT_DIM,\n",
        "                                    output_dim = Z_DIM, \n",
        "                                    conv_filters = [32, 64, 64, 64],\n",
        "                                    conv_kernel_size = [3,3,3,3],\n",
        "                                    conv_strides = [2,2,2,2])\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrZYdOMO5Xsu"
      },
      "source": [
        "\n",
        "# Decoder\n",
        "def build_decoder(input_dim, shape_before_flattening, conv_filters, conv_kernel_size, \n",
        "                  conv_strides):\n",
        "\n",
        "  # Number of Conv layers\n",
        "  n_layers = len(conv_filters)\n",
        "\n",
        "  # Define model input\n",
        "  decoder_input = Input(shape = (input_dim,) , name = 'decoder_input')\n",
        "\n",
        "  # To get an exact mirror image of the encoder\n",
        "  x = Dense(np.prod(shape_before_flattening))(decoder_input)\n",
        "  x = Reshape(shape_before_flattening)(x)\n",
        "\n",
        "  # Add convolutional layers\n",
        "  for i in range(n_layers):\n",
        "      x = Conv2DTranspose(filters = conv_filters[i], \n",
        "                  kernel_size = conv_kernel_size[i],\n",
        "                  strides = conv_strides[i], \n",
        "                  padding = 'same',\n",
        "                  name = 'decoder_conv_' + str(i)\n",
        "                  )(x)\n",
        "      \n",
        "      # Adding a sigmoid layer at the end to restrict the outputs \n",
        "      # between 0 and 1\n",
        "      if i < n_layers - 1:\n",
        "        x = LeakyReLU()(x)\n",
        "      else:\n",
        "        x = Activation('sigmoid')(x)\n",
        "\n",
        "  # Define model output\n",
        "  decoder_output = x\n",
        "\n",
        "  return decoder_input, decoder_output, Model(decoder_input, decoder_output)\n",
        "\n",
        "\n",
        "\n",
        "vae_decoder_input, vae_decoder_output, vae_decoder = build_decoder(input_dim = Z_DIM,\n",
        "                                        shape_before_flattening = vae_shape_before_flattening,\n",
        "                                        conv_filters = [64,64,32,3],\n",
        "                                        conv_kernel_size = [3,3,3,3],\n",
        "                                        conv_strides = [2,2,2,2]\n",
        "                                        )"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QV_mVyId5X--"
      },
      "source": [
        "\n",
        "# The input to the model will be the image fed to the encoder.\n",
        "vae_input = vae_encoder_input\n",
        "\n",
        "# Output will be the output of the decoder. The term - decoder(encoder_output) \n",
        "# combines the model by passing the encoder output to the input of the decoder.\n",
        "vae_output = vae_decoder(vae_encoder_output)\n",
        "\n",
        "# Input to the combined model will be the input to the encoder.\n",
        "# Output of the combined model will be the output of the decoder.\n",
        "\n",
        "vae_model = Model(vae_input, vae_output)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuIQuVd96vXk",
        "outputId": "510e43a7-6eaf-4d8f-a791-f5571b2c965a"
      },
      "source": [
        "LEARNING_RATE = 0.0005\n",
        "N_EPOCHS = 200\n",
        "LOSS_FACTOR = 10000\n",
        "\n",
        "def r_loss(y_true, y_pred):\n",
        "    return K.mean(K.square(y_true - y_pred), axis = [1,2,3])\n",
        "\n",
        "def kl_loss(y_true, y_pred):\n",
        "    kl_loss =  -0.5 * K.sum(1 + log_var - K.square(mean_mu) - K.exp(log_var), axis = 1)\n",
        "    return kl_loss\n",
        "\n",
        "def total_loss(y_true, y_pred):\n",
        "    return LOSS_FACTOR*r_loss(y_true, y_pred) + kl_loss(y_true, y_pred)\n",
        "  \n",
        "adam_optimizer = Adam(lr = LEARNING_RATE)\n",
        "\n",
        "vae_model.compile(optimizer=adam_optimizer,\n",
        "                  loss = \"mse\")\n",
        "\n",
        "\n",
        "checkpoint_vae = ModelCheckpoint(os.path.join(WEIGHTS_FOLDER, 'VAE/weights.h5'), save_weights_only = True, verbose=1)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "3MmkhvQABBbz",
        "outputId": "68ac89a9-8386-4507-b913-6e2c6b640eae"
      },
      "source": [
        "\n",
        "vae_model.summary()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e25b143d41c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvae_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'vae_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6tedYeU67JG",
        "outputId": "e2f659aa-356d-4747-f645-2cc299ae87ec"
      },
      "source": [
        "vae_model.fit(data_flow, \n",
        "                        shuffle=True, \n",
        "                        epochs = N_EPOCHS, \n",
        "                        initial_epoch = 0, \n",
        "                        steps_per_epoch=NUM_IMAGES / BATCH_SIZE,\n",
        "                        callbacks=[checkpoint_vae])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "396/395 [==============================] - ETA: 0s - loss: 0.0282\n",
            "Epoch 00001: saving model to ./weights/VAE/weights.h5\n",
            "395/395 [==============================] - 335s 819ms/step - loss: 0.0282\n",
            "Epoch 2/200\n",
            "396/395 [==============================] - ETA: 0s - loss: 0.0114\n",
            "Epoch 00002: saving model to ./weights/VAE/weights.h5\n",
            "395/395 [==============================] - 306s 773ms/step - loss: 0.0114\n",
            "Epoch 3/200\n",
            "396/395 [==============================] - ETA: 0s - loss: 0.0095\n",
            "Epoch 00003: saving model to ./weights/VAE/weights.h5\n",
            "395/395 [==============================] - 300s 757ms/step - loss: 0.0095\n",
            "Epoch 4/200\n",
            "396/395 [==============================] - ETA: 0s - loss: 0.0087\n",
            "Epoch 00004: saving model to ./weights/VAE/weights.h5\n",
            "395/395 [==============================] - 298s 752ms/step - loss: 0.0087\n",
            "Epoch 5/200\n",
            "268/395 [===================>..........] - ETA: 1:35 - loss: 0.0083"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eOFlJa1BNS1"
      },
      "source": [
        "def vae_generate_images(n_to_show=10):\n",
        "  reconst_images = vae_decoder.predict(np.random.normal(0,1,size=(n_to_show,Z_DIM)))\n",
        "\n",
        "  fig = plt.figure(figsize=(15, 3))\n",
        "  fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
        "\n",
        "  for i in range(n_to_show):\n",
        "        img = reconst_images[i].squeeze()\n",
        "        sub = fig.add_subplot(2, n_to_show, i+1)\n",
        "        sub.axis('off')        \n",
        "        sub.imshow(img)\n",
        "\n",
        "vae_generate_images(n_to_show=10)  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}